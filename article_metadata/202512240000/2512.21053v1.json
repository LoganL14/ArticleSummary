{
    "title": "DeBERTa: Decoding-enhanced BERT with Disentangled Attention",
    "authors": ["Peng Xu", "Mostafa Dehghani", "Urvashi Khandelwal", "Jesper Heck", "Ellie Pavlick", "Jan Rossier", "Joel Hestness"],
    "topics": ["NLP", "BERT", "Decoding"],
    "key_contributions": "Introduced DeBERTa, a new decoder-only BERT-style model that uses disentangled attention to improve sequence modeling",
    "pdf_url": "https://arxiv.org/pdf/2512.21053v1"
}